<!DOCTYPE html>
<html lang="th">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Mediapipe JS</title>
<style>
  body { text-align: center; background: #222; color: #fff; font-family: sans-serif; }
  h2 { margin-bottom: 20px; }
  .camera-container { display: flex; justify-content: center; gap: 20px; }
  video, canvas {
    width: 480px;
    height: 270px;
    border: 1px solid #ccc;
    background: #000;
  }
</style>
</head>
<body>
<h2>ตรวจจับท่าทางด้วย Mediapipe</h2>
<div class="camera-container">
  <video id="oriVideo" autoplay playsinline></video>
  <canvas id="poseCanvas" width="1920" height="1080"></canvas>
</div>

<script src="https://cdn.jsdelivr.net/npm/@mediapipe/pose/pose.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>

<script type="module">

import { loadTemplate } from "./templateLoader.js"
import { addFrame, getUserSequence } from './suquenceManager.js';
import { computeDTW } from './dtw.js';
import { displayScore } from './dtwUI.js';

const oriVideo = document.getElementById("oriVideo");
const poseCanvas = document.getElementById("poseCanvas");
const ctx = poseCanvas.getContext("2d");

const pose = new Pose({
  locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/pose/${file}`
});

pose.setOptions({
  modelComplexity: 1,
  smoothLandmarks: true,
  enableSegmentation: false,
  minDetectionConfidence: 0.5,
  minTrackingConfidence: 0.5
});

//DTW template
let templateLandmarks = null;
loadTemplate('./choke.json').then(rawData => {
    const numFrames = rawData["Shoulder (11)"].length;
    const templateSequence = [];
    for (let i = 0; i < numFrames; i++) {
        const frame = [
            [rawData["Shoulder (11)"][i], rawData["Elbow (13)"][i], rawData["Wrist (15)"][i]]
        ];
        templateSequence.push(frame);
    }
    templateLandmarks = templateSequence;
});

pose.onResults((results) => {
  ctx.clearRect(0, 0, poseCanvas.width, poseCanvas.height);
  ctx.drawImage(results.image, 0, 0, poseCanvas.width, poseCanvas.height);
  if (results.poseLandmarks) {
    drawConnectors(ctx, results.poseLandmarks, POSE_CONNECTIONS,
                   {color: '#00FF00', lineWidth: 8});
    drawLandmarks(ctx, results.poseLandmarks,
                  {color: '#FF0000', lineWidth: 4});
  }
  addFrame(results.poseLandmarks);

  //comparison
  if (templateLandmarks) {
    const score = computeDTW(getUserSequence(), templateLandmarks);
    displayScore(ctx, score);
  }
});

async function startCam() {
  const stream = await navigator.mediaDevices.getUserMedia({
    video: { width: 640, height: 360, frameRate: 15 }
  });

  oriVideo.srcObject = stream;
  await oriVideo.play();
  requestAnimationFrame(processFrame);
}

async function processFrame() {
  const vw = oriVideo.videoWidth;
  const vh = oriVideo.videoHeight;
  const aspect = 16 / 9;
  let sx = 0, sy = 0, sw = vw, sh = vh;
  if (vw / vh > aspect) {
    sw = vh * aspect;
    sx = (vw - sw) / 2;
  } else {
    sh = vw / aspect;
    sy = (vh - sh) / 2;
  }

  const offCanvas = document.createElement('canvas');
  offCanvas.width = sw;
  offCanvas.height = sh;
  const offCtx = offCanvas.getContext('2d');
  offCtx.drawImage(oriVideo, sx, sy, sw, sh, 0, 0, sw, sh);

  await pose.send({image: offCanvas});
  requestAnimationFrame(processFrame);
}

startCam();
</script>
</body>
</html>
